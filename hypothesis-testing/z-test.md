# Z-Test

A statistical test used to determine whether population parameters (means or proportions) differ from hypothesized values or between groups when the population standard deviation is known or sample sizes are large.

## üéØ When to Use Z-Test

### ‚úÖ Use Z-Test when:

- **Comparing two means**
- **Population standard deviation is known**
- **Sample size is large (n > 30)**
- **Data is normally distributed**

### ‚ùå Don't use Z-Test when:

- Population standard deviation is unknown
- Sample size is small (n < 30)
- Data is not normally distributed

## üìä Z-Test for Means

### üîç Variance of Sample Mean Foundation

**Key Principle**: The variance of a sample mean equals the population variance divided by sample size:

$$\text{Var}(\bar{X}) = \frac{\sigma^2}{n}$$

#### üéØ Intuitive Explanation

**Why averaging reduces variance:**

- **Individual observations**: Each has high variability (œÉ¬≤)
- **Averaging effect**: Extreme values cancel out
- **Larger samples**: More cancellation ‚Üí less variability
- **Result**: Sample mean is more precise than individual observations

**Analogy**: Like taking multiple photos and averaging them to reduce noise!

#### üìê Mathematical Derivation

**Given**: $X_1, X_2, \ldots, X_n$ are independent random variables with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$

**Sample mean**: $\bar{X} = \frac{1}{n}(X_1 + X_2 + \cdots + X_n) = \frac{1}{n}\sum_{i=1}^{n} X_i$

**Step 1**: Apply variance operator
$$\text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)$$

**Step 2**: Factor out constant
$$\text{Var}(\bar{X}) = \frac{1}{n^2} \text{Var}\left(\sum_{i=1}^{n} X_i\right)$$

**Step 3**: Use independence property (key step!)
$$\text{Var}\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} \text{Var}(X_i) = n\sigma^2$$

**Step 4**: Final result
$$\text{Var}(\bar{X}) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$$

**Standard Error**: $\text{SE}(\bar{X}) = \sqrt{\text{Var}(\bar{X})} = \frac{\sigma}{\sqrt{n}}$

#### üîó Independence and Variance Addition

**General Rule for Independent Random Variables**:
$$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)$$

**When X and Y are independent**: $\text{Cov}(X, Y) = 0$

**Therefore**: $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$

**Definition of Independent Random Variables**:
Two random variables X and Y are independent if:
$$P(X = x \text{ and } Y = y) = P(X = x) \cdot P(Y = y)$$

for all possible values of x and y.

**Key Implication**: Knowledge about X provides no information about Y

**Covariance of Independent Variables**:
$$\text{Cov}(X, Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0$$

**This is why variances add for independent samples!**

### One-Sample Z-Test

**Hypotheses:**

- **H‚ÇÄ**: Œº = Œº‚ÇÄ (population mean equals hypothesized value)
- **H‚ÇÅ**: Œº ‚â† Œº‚ÇÄ (two-tailed) or Œº > Œº‚ÇÄ / Œº < Œº‚ÇÄ (one-tailed)

**Test Statistic:**

```
Z = (xÃÑ - Œº‚ÇÄ) / (œÉ/‚àön)
```

Where:

- xÃÑ = sample mean
- Œº‚ÇÄ = hypothesized population mean
- œÉ = population standard deviation (known)
- n = sample size

### Two-Sample Z-Test

**Hypotheses:**

- **H‚ÇÄ**: Œº‚ÇÅ = Œº‚ÇÇ (no difference between population means)
- **H‚ÇÅ**: Œº‚ÇÅ ‚â† Œº‚ÇÇ (two-tailed) or Œº‚ÇÅ > Œº‚ÇÇ / Œº‚ÇÅ < Œº‚ÇÇ (one-tailed)

**Test Statistic:**

```
Z = (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ) / ‚àö(œÉ‚ÇÅ¬≤/n‚ÇÅ + œÉ‚ÇÇ¬≤/n‚ÇÇ)
```

Where:

- xÃÑ‚ÇÅ, xÃÑ‚ÇÇ = sample means
- œÉ‚ÇÅ, œÉ‚ÇÇ = population standard deviations (known)
- n‚ÇÅ, n‚ÇÇ = sample sizes

**Denominator Breakdown:**

```
‚àö(œÉ‚ÇÅ¬≤/n‚ÇÅ + œÉ‚ÇÇ¬≤/n‚ÇÇ) = ‚àö(Var(xÃÑ‚ÇÅ) + Var(xÃÑ‚ÇÇ))
```

- œÉ‚ÇÅ¬≤/n‚ÇÅ = Variance of sample mean 1
- œÉ‚ÇÇ¬≤/n‚ÇÇ = Variance of sample mean 2
- Addition because samples are independent
- Square root converts variance to standard error

## üìà Z-Test for Proportions

### When to Use

- **Comparing success rates** (one sample vs hypothesis, or two groups)
- **Large enough sample sizes** (rule of thumb: np ‚â• 5 and n(1-p) ‚â• 5)
- **Binary outcome** (success/failure)

### Parameter Definitions

- **p** = True population proportion (unknown parameter)
- **q** = 1 - p (probability of failure)
- **pÃÇ** = Sample proportion = x/n (observed)
- **p‚ÇÅ, p‚ÇÇ** = True proportions in populations 1 and 2
- **pÃÇ‚ÇÅ, pÃÇ‚ÇÇ** = Sample proportions from groups 1 and 2

### One-Sample Z-Test for Proportions

**Hypotheses:**

- **H‚ÇÄ**: p = p‚ÇÄ (population proportion equals hypothesized value)
- **H‚ÇÅ**: p ‚â† p‚ÇÄ (two-tailed) or p > p‚ÇÄ / p < p‚ÇÄ (one-tailed)

**Test Statistic:**

```
Z = (pÃÇ - p‚ÇÄ) / ‚àö(p‚ÇÄ(1-p‚ÇÄ)/n)
```

Where:

- pÃÇ = x/n = observed sample proportion
- p‚ÇÄ = hypothesized population proportion
- n = sample size
- ‚àö(p‚ÇÄ(1-p‚ÇÄ)/n) = standard error under null hypothesis

### Two-Sample Z-Test for Proportions

**Hypotheses:**

- **H‚ÇÄ**: p‚ÇÅ = p‚ÇÇ (no difference in population proportions)
- **H‚ÇÅ**: p‚ÇÅ ‚â† p‚ÇÇ (two-tailed) or p‚ÇÅ > p‚ÇÇ / p‚ÇÅ < p‚ÇÇ (one-tailed)

**Test Statistic:**

```
Z = (pÃÇ‚ÇÅ - pÃÇ‚ÇÇ) / ‚àö(pÃÇ(1-pÃÇ)(1/n‚ÇÅ + 1/n‚ÇÇ))
```

Where:

- pÃÇ‚ÇÅ = x‚ÇÅ/n‚ÇÅ = sample proportion from group 1
- pÃÇ‚ÇÇ = x‚ÇÇ/n‚ÇÇ = sample proportion from group 2
- pÃÇ = (x‚ÇÅ + x‚ÇÇ)/(n‚ÇÅ + n‚ÇÇ) = pooled proportion (under H‚ÇÄ: p‚ÇÅ = p‚ÇÇ)
- x‚ÇÅ, x‚ÇÇ = number of successes in each group
- n‚ÇÅ, n‚ÇÇ = sample sizes

**Why Pooled Proportion?**
Under H‚ÇÄ, both groups have same proportion, so we pool data for best estimate.

## üîç Decision Making

### Critical Values

- **Two-tailed test**: ¬±1.96 (Œ± = 0.05), ¬±2.58 (Œ± = 0.01)
- **One-tailed test**: 1.645 (Œ± = 0.05), 2.326 (Œ± = 0.01)

### Decision Rule

- If |Z| > critical value: **Reject H‚ÇÄ**
- If |Z| ‚â§ critical value: **Fail to reject H‚ÇÄ**

## üìù Example: Comparing Success Rates

**Question**: "Does the rate of success differ across two groups?"

### Steps:

1. **Check assumptions**: Large sample sizes, binary outcome
2. **State hypotheses**:
   - H‚ÇÄ: p‚ÇÅ = p‚ÇÇ
   - H‚ÇÅ: p‚ÇÅ ‚â† p‚ÇÇ
3. **Calculate test statistic**
4. **Compare to critical value**
5. **Make decision and conclusion**

## üîó Related Tests

- **[T-Test](./t-test.md)** - When population standard deviation is unknown
- **[Chi-Square Test](./chi-square-test.md)** - For categorical variables in contingency tables
- **[Fisher's Exact Test](./chi-square-test.md#fishers-exact-test)** - For small sample sizes

## üéØ Z-Test vs T-Test: When to Use Which?

### Why Z-Test for Known Population Variance?

**Z-Test Conditions:**

- **Population standard deviation (œÉ) is known**
- **Large sample sizes (n > 30)** - Central Limit Theorem ensures normality
- **Sampling distribution is exactly normal**

**Mathematical Reason:**

```
Z = (xÃÑ - Œº‚ÇÄ) / (œÉ/‚àön)  ‚Üê œÉ is known, so denominator is fixed
```

- No estimation error in denominator
- Test statistic follows standard normal distribution exactly

### Why T-Test for Unknown Population Variance?

**T-Test Conditions:**

- **Population standard deviation (œÉ) is unknown**
- **Must estimate œÉ using sample standard deviation (s)**
- **Additional uncertainty from estimation**

**Mathematical Reason:**

```
t = (xÃÑ - Œº‚ÇÄ) / (s/‚àön)  ‚Üê s is estimated, adds uncertainty
```

- Estimation error in denominator creates extra variability
- Test statistic follows t-distribution (heavier tails than normal)
- Degrees of freedom = n-1

## üíº Financial Industry Examples

### When Population Variance is KNOWN (Use Z-Test)

**1. Market Returns with Established Volatility:**

```python
# S&P 500 daily returns - historical volatility well-established
# œÉ = 1.2% daily (from decades of data)
# Test if recent 30-day average return differs from 0

sigma_known = 0.012  # Known from historical data
n = 30
sample_mean = 0.008
hypothesized_mean = 0

z_stat = (sample_mean - hypothesized_mean) / (sigma_known / np.sqrt(n))
# Z-test appropriate because œÉ is well-established
```

**2. High-Frequency Trading Latency:**

```python
# Network latency for trading systems
# œÉ = 2.5ms (known from infrastructure specifications)
# Test if new system has different average latency

sigma_known = 2.5  # Known from system specs
n = 1000  # Large sample of trades
sample_mean = 12.3
hypothesized_mean = 12.0

z_stat = (sample_mean - hypothesized_mean) / (sigma_known / np.sqrt(n))
```

**3. Currency Exchange Rate Volatility:**

```python
# EUR/USD daily volatility - well-documented
# œÉ = 0.8% (from years of FX market data)
# Compare volatility between two time periods

sigma1_known = 0.008  # Historical volatility
sigma2_known = 0.008  # Assuming same underlying process
n1, n2 = 50, 60
mean1, mean2 = 0.0012, 0.0018

z_stat = (mean1 - mean2) / np.sqrt(sigma1_known**2/n1 + sigma2_known**2/n2)
```

### When Population Variance is UNKNOWN (Use T-Test)

**1. New Financial Product Performance:**

```python
# New cryptocurrency with limited history
# No established volatility pattern
# Must estimate œÉ from sample data

sample_returns = [0.02, -0.01, 0.03, 0.01, -0.02, ...]  # Limited data
n = len(sample_returns)
sample_mean = np.mean(sample_returns)
sample_std = np.std(sample_returns, ddof=1)  # Estimate œÉ
hypothesized_mean = 0

t_stat = (sample_mean - hypothesized_mean) / (sample_std / np.sqrt(n))
# T-test because we're estimating œÉ from limited data
```

**2. Private Equity Fund Returns:**

```python
# Small sample of quarterly returns
# No established volatility benchmark
# Population variance unknown

quarterly_returns = [0.05, 0.08, -0.02, 0.12, 0.03]  # Only 5 quarters
n = len(quarterly_returns)
sample_mean = np.mean(quarterly_returns)
sample_std = np.std(quarterly_returns, ddof=1)
hypothesized_mean = 0.06  # Target return

t_stat = (sample_mean - hypothesized_mean) / (sample_std / np.sqrt(n))
# T-test due to small sample and unknown population variance
```

**3. Credit Default Rates for New Loan Product:**

```python
# New lending product with no historical data
# Default rate variance unknown
# Must estimate from initial sample

defaults = [1, 0, 0, 1, 0, 1, 0, 0, 1, 0]  # Binary outcomes
n = len(defaults)
sample_proportion = np.mean(defaults)
sample_std = np.sqrt(sample_proportion * (1 - sample_proportion))  # Estimated
hypothesized_rate = 0.05

# Would use t-test for proportion or exact binomial test
# because population variance is unknown
```

## üí° Key Insights

1. **Z-Test**: Use when œÉ is truly known from extensive historical data or theoretical models
2. **T-Test**: Use when œÉ must be estimated from sample (most real-world situations)
3. **Large samples**: Z and t converge, but t is still more conservative
4. **Financial markets**: Established instruments ‚Üí Z-test; New products ‚Üí t-test
5. **Risk management**: Known volatility models ‚Üí Z-test; Estimated parameters ‚Üí t-test

---

_‚Üê [Hypothesis Testing Overview](./README.md) | [T-Test](./t-test.md) ‚Üí_
